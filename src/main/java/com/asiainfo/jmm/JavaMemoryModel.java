package com.asiainfo.jmm;

/**
 * Java内存模型：
 * JMM决定一个线程对共享变量的写入何时对另一个线程可见。从抽象的角度来看，JMM定义了线程和主内存之间的抽象关系：
 * 线程之间的共享变量存储在主内存（main memory）中，每个线程都有一个私有的本地内存（local memory通常是cpu缓存、寄存器），本地内存中存储了该线程已读/写共享变量的副本。
 * 本地内存是JMM的一个抽象概念，并不真实存在。它涵盖了缓存，写缓冲区，寄存器以及其他的硬件和编译器优化。
 * 
 * 			线程A					线程B
 * 
 * 			  ^						  ^
 * 			  |						  |
 * 			  V						  V
 * 
 * 		  本地内存A				   本地内存B
 * 		 共享变量的拷贝		        共享变量的拷贝
 * 
 * 			  ^						  ^
 * 			  |			JMM控制		  |
 * 			  V						  V		
 *  
 * 						主内存
 * 			共享变量1、共享变量2、共享变量3 ...
 * 
 * 
 * 线程A与线程B之间如要通信的话，必须要经历下面2个步骤：
 * 1. 首先，线程A把本地内存A中更新过的共享变量刷新到主内存中去。
 * 2. 然后，线程B到主内存中去读取线程A之前已更新过的共享变量。 
 * 
 * JMM通过控制主内存与每个线程的本地内存之间的交互，来为java程序员提供内存可见性保证。
 * 
 * 在JVM内部，Java内存模型把内存分成了两部分：线程栈区stack和堆区heap。
 * 线程栈包含了当前线程执行的方法调用相关信息，线程栈包含了当前方法的所有局部变量信息，所有原始类型(boolean,byte,short,char,int,long,float,double)的
 * 局部变量都直接保存在线程栈当中，对于它们的值各个线程之间都是独立的。对于原始类型的局部变量，一个线程可以传递一个副本给另一个线程，它们之间是无法共享的。
 * 
 * 堆区包含了Java应用创建的所有对象信息，不管对象是哪个线程创建的，其中的对象包括原始类型的封装类（如Byte、Integer、Long等等）。
 * 不管对象是属于一个成员变量还是方法中的局部变量，它都会被存储在堆区。
 * 一个局部变量如果是原始类型，那么它会被完全存储到栈区。 一个局部变量也有可能是一个对象的引用，这种情况下，这个局部引用会被存储到栈中，但是对象本身仍然存储在堆区。
 * 对于一个对象的成员方法，这些方法中包含局部变量，仍需要存储在栈区，即使它们所属的对象在堆区。 
 * 对于一个对象的成员变量，不管它是原始类型还是包装类型，都会被存储到堆区。
 * static类型的变量以及类型本身相关信息都会随着类本身存储在堆区。
 * 
 * 
 * 
 * 硬件内存架构：
 * 
 * 			 CPU					 CPU	
 * 		  CPU 寄存器				  CPU 寄存器
 * 
 * 			  ^						  ^
 * 			  |						  |
 * 			  V						  V
 * 		  CPU Cache				  CPU Cache
 * 		    Memory					Memory
 * 
 * 			  ^						  ^
 * 			  |						  |
 * 			  V						  V
 * 
 * 				  RAM - Main Memory
 * 
 * 现代计算机一般都有2个以上CPU，而且每个CPU还有可能包含多个核心。因此，如果我们的应用是多线程的话，这些线程可能会在各个CPU核心中并行运行。
 * 在CPU内部有一组CPU寄存器，也就是CPU的储存器。CPU操作寄存器的速度要比操作计算机主存快的多。
 * 在主存和CPU寄存器之间还存在一个CPU缓存，CPU操作CPU缓存的速度快于主存但慢于CPU寄存器。某些CPU可能有多个缓存层（一级缓存和二级缓存）。
 * 计算机的主存也称作RAM，所有的CPU都能够访问主存，而且主存比上面提到的缓存和寄存器大很多。
 * 
 * 当一个CPU需要访问主存时，会先读取一部分主存数据到CPU缓存，进而在读取CPU缓存到寄存器。
 * 当CPU需要写数据到主存时，同样会先flush寄存器到CPU缓存，然后再在某些节点把缓存数据flush到主存。
 * 由于现代的处理器都会使用写缓冲区，因此现代的处理器都会允许对写-读操作做重排序。
 * 
 * 
 * 常见处理器允许的重排序类型的列表：
 *              Load-Load   Load-Store  Store-Store Store-Load  数据依赖
 *  sparc-TSO       N           N           N           Y           N
 *  x86             N           N           N           Y           N
 *  ia64            Y           Y           Y           Y           N
 *  PowerPC/ARM     Y           Y           Y           Y           N
 *  注1：sparc-TSO是指以TSO(Total Store Order)内存模型运行时，sparc处理器的特性。
 *  注2：x86包括x64及AMD64。
 *  注3：如果两个操作访问同一个变量，且这两个操作中有一个为写操作，此时这两个操作之间就存在数据依赖性。
 *      
 *  
 * 常见的处理器都允许Store-Load重排序；常见的处理器都不允许对存在数据依赖的操作做重排序。
 * sparc-TSO和x86拥有相对较强的处理器内存模型，它们仅允许对写-读操作做重排序（因为它们都使用了写缓冲区）。
 *  
 * 
 * 数据竞争: 当程序未正确同步时，就会存在数据竞争。
 * java内存模型规范对数据竞争的定义：在一个线程中写一个变量，在另一个线程读同一个变量，而且写和读没有通过同步来排序。
 * JMM对正确同步的多线程程序的内存一致性做了如下保证：
 *   如果程序是正确同步的，程序的执行将具有顺序一致性（sequentially consistent）-- 即程序的执行结果与该程序在顺序一致性内存模型中的执行结果相同。
 *   这里的同步是指广义上的同步，包括对常用同步原语（lock，volatile和final）的正确使用。
 * 
 * 顺序一致性:
 *   在顺序一致性模型中，所有操作完全按程序的顺序串行执行。而在JMM中，临界区内的代码可以重排序（但JMM不允许临界区内的代码“逸出”到临界区之外，那样会破坏监视器的语义）。
 *   JMM会在退出监视器和进入监视器这两个关键时间点做一些特别处理，使得线程在这两个时间点具有与顺序一致性模型相同的内存视图。
 *   
 * 未同步程序的执行特性:
 *   未同步程序在JMM中的执行时，整体上也是无序的，其执行结果也无法预知。
 *   JMM不保证对64位的long型和double型变量的读/写操作具有原子性。
 *   对于未同步或未正确同步的多线程程序，JMM只提供最小安全性：线程执行时读取到的值，要么是之前某个线程写入的值，要么是默认值（0，null，false），
 *   JMM保证线程读操作读取到的值不会无中生有（out of thin air）的冒出来。
 *   为了实现最小安全性，JVM在堆上分配对象时，首先会清零内存空间，然后才会在上面分配对象（JVM内部会同步这两个操作）。
 *   因此，在以清零的内存空间（pre-zeroed memory）分配对象时，域的默认初始化已经完成了。
 * 
 * 读/写操作原子性:
 *   在计算机中，数据通过总线在处理器和内存之间传递。每次处理器和内存之间的数据传递都是通过一系列步骤来完成的，这一系列步骤称之为总线事务（bus transaction）。
 *   总线事务包括读事务（read transaction）和写事务（write transaction）。读事务从内存传送数据到处理器，写事务从处理器传送数据到内存，
 *   每个事务会读/写内存中一个或多个物理上连续的字。这里的关键是，总线会同步试图并发使用总线的事务。
 *   在一个处理器执行总线事务期间，总线会禁止其它所有的处理器和I/O设备执行内存的读/写。总线的这些工作机制可以把所有处理器对内存的访问以串行化的方式来执行；
 *   在任意时间点，最多只能有一个处理器能访问内存。这个特性确保了单个总线事务之中的内存读/写操作具有原子性。
 * 32位处理器：
 *   在一些32位的处理器上，如果要求对64位数据的读/写操作具有原子性，会有比较大的开销。
 *   为了照顾这种处理器，java语言规范鼓励但不强求JVM对64位的long型变量和double型变量的读/写具有原子性。
 *   当JVM在这种处理器上运行时，会把一个64位long/ double型变量的读/写操作拆分为两个32位的读/写操作来执行。
 *   这两个32位的读/写操作可能会被分配到不同的总线事务中执行，此时对这个64位变量的读/写将不具有原子性。
 * 
 * 
 * Java内存模型和硬件架构之间的桥接:
 * Java内存模型和硬件内存架构并不一致。硬件内存架构中并没有区分栈和堆，从硬件上看，不管是栈还是堆，大部分数据都会存到主存中，
 * 当然一部分栈和堆的数据也有可能会存到CPU寄存器中。
 * 
 * 当对象和变量存储到计算机的各个内存区域时，必然会面临一些问题，其中最主要的两个问题是：
 * 1. 共享对象对各个线程的可见性
 * 	    当多个线程同时操作同一个共享对象时，如果没有合理的使用volatile和synchronized关键字，一个线程对共享对象的更新有可能导致其它线程不可见。
 * 	  volatile 关键字可以保证变量会直接从主存读取，而对变量的更新也会直接写到主存。volatile原理是基于CPU内存屏障指令实现的。
 * 2. 共享对象的竞争现象
 * 	   多个线程共享一个对象，如果它们同时修改这个共享对象，这就产生了竞争现象。
 *   synchronized代码块可以保证同一个时刻只能有一个线程进入代码竞争区，synchronized代码块也能保证代码块中所有变量都将会从主存中读，
 *   当线程退出代码块时，对所有变量的更新将会flush到主存，不管这些变量是不是volatile类型的。
 *   
 *   
 * 
 * 支撑Java内存模型的基础原理：
 * 1. 指令重排序
 *    在计算机中，软件技术和硬件技术有一个共同的目标：在不改变程序执行结果的前提下，尽可能的开发并行度。
 *    在执行程序时，为了提高性能，编译器和处理器会对指令做重排序。但是，JMM确保在不同的编译器和不同的处理器平台之上，通过插入特定类型的Memory Barrier来
 *    禁止特定类型的编译器重排序和处理器重排序，为上层提供一致的内存可见性保证。
 *    a. 编译器优化重排序：编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。
 *    b. 指令级并行重排序：现代处理器采用了指令级并行技术（Instruction-Level Parallelism， ILP）来将多条指令重叠执行。
 *                       如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。
 *    c. 内存系统重排序：由于处理器使用缓存和读写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。
 *    从java源代码到最终实际执行的指令序列，会分别经历下面三种重排序：
 *    
 *    java源代码  -> 编译器优化重排序 -> 指令级并行重排序 -> 内存系统重排序 -> 最终指向的指令序列
 *    
 *    这些重排序都可能会导致多线程程序出现内存可见性问题。
 *    对于编译器，JMM的编译器重排序规则会禁止特定类型的编译器重排序。
 *    对于处理器重排序，JMM的处理器重排序规则会要求java编译器在生成指令序列时，插入特定类型的内存屏障（memory barriers，intel称之为memory fence）指令，
 *    通过内存屏障指令来禁止特定类型的处理器重排序（不是所有的处理器重排序都要禁止）。
 *    JMM属于语言级的内存模型，它确保在不同的编译器和不同的处理器平台之上，通过禁止特定类型的编译器重排序和处理器重排序，为程序员提供一致的内存可见性保证。
 *    
 *    数据依赖性: 如果两个操作访问同一个变量，其中一个为写操作，此时这两个操作之间存在数据依赖性。 
 *    			 编译器和处理器不会改变存在数据依赖性关系的两个操作的执行顺序，即不会重排序。
 *              数据依赖操作包括：写后读  【a = 1;b = a;】、写后写  【a = 1;a = 2;】、读后写  【a = b;b = 1;】
 * 	  as-if-serial: 不管怎么重排序，单线程下的执行结果不能被改变，编译器、runtime和处理器都必须遵守as-if-serial语义。
 * 
 * 2. 内存屏障（Memory Barrier）
 * 	    通过内存屏障可以禁止特定类型处理器的重排序，从而让程序按我们预想的流程去执行。内存屏障，又称内存栅栏，是一个CPU指令，
 * 	    它是一条这样的指令：
 * 	  a. 保证特定操作的执行顺序。
 * 	  b. 影响某些数据（或则是某条指令的执行结果）的内存可见性。
 * 
 * 	    编译器和CPU能够重排序指令，保证最终相同的结果，尝试优化性能。插入一条Memory Barrier会告诉编译器和CPU：不管什么指令都不能和这条Memory Barrier指令重排序。
 *    Memory Barrier所做的另外一件事是强制刷出各种CPU cache，如一个Write-Barrier（写入屏障）将刷出所有在Barrier之前写入 cache 的数据，
 *    因此，任何CPU上的线程都能读取到这些数据的最新版本。
 *    
 *    如果一个变量是volatile修饰的，JMM会在写入这个字段之后插进一个Write-Barrier指令，并在读这个字段之前插入一个Read-Barrier指令。
 *    这意味着，如果写入一个volatile变量，就可以保证：
 *    a. 一个线程写入变量a后，任何线程访问该变量都会拿到最新值。
 *    b. 在写入变量a之前的写入操作，其更新的数据对于其他线程也是可见的。因为Memory Barrier会刷出cache中的所有先前的写入。
 *    
 *    JMM把内存屏障指令分为下列四类：
 *    屏障类型					指令示例							说明
 *    LoadLoad Barriers			Load1; LoadLoad; Load2			确保Load1数据的装载，之前于Load2及所有后续装载指令的装载。
 *    StoreStore Barriers		Store1; StoreStore; Store2		确保Store1数据对其他处理器可见（刷新到内存），之前于Store2及所有后续存储指令的存储。
 *    LoadStore Barriers		Load1; LoadStore; Store2		确保Load1数据装载，之前于Store2及所有后续的存储指令刷新到内存。
 *    StoreLoad Barriers		Store1; StoreLoad; Load2		确保Store1数据对其他处理器变得可见（指刷新到内存），之前于Load2及所有后续装载指令的装载。
 *    															StoreLoad会使该屏障之前的所有内存访问指令（存储和装载指令）完成之后，才执行该屏障之后的内存访问指令。  
 *    StoreLoad Barriers是一个“全能型”的屏障，它同时具有其他三个屏障的效果。现代的多处理器大都支持该屏障（其他类型的屏障不一定被所有处理器支持）。
 *    执行该屏障开销会很昂贵，因为当前处理器通常要把写缓冲区中的数据全部刷新到内存中（buffer fully flush）。
 *    
 *    需要的屏障	
 *    第一个操作v        第二个操作>
 *    -----------------------------------------------------------------------------------------------------------------------------
 *    				  Normal Load			Normal Store		Volatile Load/Monitor Enter				Volatile Store/Monitor Exit
 *    -----------------------------------------------------------------------------------------------------------------------------
 *    Normal Load																						LoadStore
 *    -----------------------------------------------------------------------------------------------------------------------------
 *    Normal Store																						StoreStore
 *    -----------------------------------------------------------------------------------------------------------------------------
 *    VLoad/MEnter		LoadLoad			LoadStore			LoadLoad								LoadStore
 *    -----------------------------------------------------------------------------------------------------------------------------
 *    VStore/MExit												StoreLoad								StoreStore
 *    -----------------------------------------------------------------------------------------------------------------------------
 *    为了实现volatile的内存语义，编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。
 *    对于编译器来说，发现一个最优布置来最小化插入屏障的总数几乎不可能，为此，JMM采取保守策略。
 *    下面是基于保守策略的JMM内存屏障插入策略：
 *      在每个volatile写操作的前面插入一个StoreStore屏障。
 *      在每个volatile写操作的后面插入一个StoreLoad屏障。
 *      在每个volatile读操作的后面插入一个LoadLoad屏障。
 *      在每个volatile读操作的后面插入一个LoadStore屏障。
 *      
 *    x86处理器仅会对写-读操作做重排序。X86不会对读-读，读-写和写-写操作做重排序，因此在x86处理器中会省略掉这三种操作类型对应的内存屏障。
 *    在x86中，JMM仅需在volatile写后面插入一个StoreLoad屏障即可正确实现volatile写-读的内存语义。
 *    这意味着在x86处理器中，volatile写的开销比volatile读的开销会大很多（因为执行StoreLoad屏障开销会比较大）。
 *    
 *    
 *    happens-before： 从jdk5开始，java使用新的JSR-133内存模型，基于happens-before的概念来阐述操作之间的内存可见性。
 *    在JMM中，如果一个操作的执行结果需要对另一个操作可见，那么这两个操作之间必须要存在happens-before关系，这个的两个操作既可以在同一个线程，也可以在不同的两个线程中。
 * 	    与程序员密切相关的happens-before规则如下：
 *    a. 程序顺序规则：一个线程中的每个操作，happens-before于该线程中任意的后续操作。
 *    b. 监视器锁规则：对一个锁的解锁操作，happens-before于随后对这个锁的加锁操作。
 *    c. volatile域规则：对一个volatile域的写操作，happens-before于任意线程后续对这个volatile域的读。
 *    d. 传递性规则：如果 A happens-before B，且 B happens-before C，那么A happens-before C。
 *    注意：两个操作之间具有happens-before关系，并不意味前一个操作必须要在后一个操作之前执行！仅仅要求前一个操作的执行结果，对于后一个操作是可见的，
 *         且前一个操作按顺序排在后一个操作之前。
 *    线程上调用start()方法happens before这个线程启动后的任何操作。
 *    一个线程中所有的操作都happens before从这个线程join()方法成功返回的任何其他线程。
 *    （注意思是其他线程等待一个线程的jion()方法完成，那么，这个线程中的所有操作happens before其他线程中的所有操作）
 * 
 * 
 * 对一个volatile变量的单个读/写操作，与对一个普通变量的读/写操作使用同一个监视器锁来同步，它们之间的执行效果相同。
 * 监视器锁的语义决定了临界区代码的执行具有原子性。这意味着即使是64位的long型和double型变量，只要它是volatile变量，对该变量的读写就将具有原子性。
 * 如果是多个volatile操作或类似于volatile++这种复合操作，这些操作整体上不具有原子性。
 * volatile变量自身具有下列特性：
 *  1. 可见性。对一个volatile变量的读，总是能看到（任意线程）对这个volatile变量最后的写入。
 *  2. 原子性：对任意单个volatile变量的读/写具有原子性，但类似于volatile++这种复合操作不具有原子性。

 * volatile写的内存语义：当写一个volatile变量时，JMM会把该线程对应的本地内存中的共享变量刷新到主内存。
 * volatile读的内存语义：当读一个volatile变量时，JMM会把该线程对应的本地内存置为无效。线程接下来将从主内存中读取共享变量。
 * 
 * 
 * 锁释放和获取的内存语义: 
 * 1. 当线程释放锁时，JMM会把该线程对应的本地内存中的共享变量刷新到主内存中。
 * 2. 当线程获取锁时，JMM会把该线程对应的本地内存置为无效。从而使得被监视器保护的临界区代码必须要从主内存中去读取共享变量。
 * 
 * 
 * java的CAS同时具有 volatile 读和volatile写的内存语义。
 * Java线程之间的通信有下面四种方式：
 * 1. A线程写volatile变量，随后B线程读这个volatile变量。
 * 2. A线程写volatile变量，随后B线程用CAS更新这个volatile变量。
 * 3. A线程用CAS更新一个volatile变量，随后B线程用CAS更新这个volatile变量。
 * 4. A线程用CAS更新一个volatile变量，随后B线程读这个volatile变量。
 * 
 * concurrent包的源代码实现，有一个通用化的实现模式：
 * 1. 首先，声明共享变量为volatile；
 * 2. 然后，使用CAS的原子条件更新来实现线程之间的同步；
 * 3. 同时，配合以volatile的读/写和CAS所具有的volatile读和写的内存语义来实现线程之间的通信。
 * 
 * 
 * volatile和synchronized的区别:
 * volatile本质是在告诉jvm当前变量在寄存器（工作内存）中的值是不确定的，需要从主存中读取； synchronized则是锁定当前变量，只有当前线程可以访问该变量，其他线程被阻塞住。
 * volatile仅能使用在变量级别；synchronized则可以使用在变量、方法、和类级别的。
 * volatile仅能实现变量的修改可见性，不能保证原子性；而synchronized则可以保证变量的修改可见性和原子性。
 * volatile不会造成线程的阻塞；synchronized可能会造成线程的阻塞。
 * volatile标记的变量不会被编译器优化；synchronized标记的变量可以被编译器优化。
 * 写入一个volatile字段和释放监视器有相同的内存影响，而且读取volatile字段和获取监视器也有相同的内存影响。
 * 
 * 同步保证了一个线程在同步块之前或者在同步块中的一个内存写入操作以可预知的方式对其他有相同监视器的线程可见。
 * 当我们退出了同步块，我们就释放了这个监视器，这个监视器有刷新缓冲区到主内存的效果，因此该线程的写入操作能够为其他线程所见。
 * 在我们进入一个同步块之前，我们需要获取监视器，监视器有使本地处理器缓存失效的功能，因此变量会从主存重新加载，于是其它线程对共享变量的修改对当前线程来说就变得可见了。
 * 
 * 
 * final语义在处理器中的实现:
 * 写final域的重排序规则会要求译编器在final域的写之后，构造函数return之前，插入一个StoreStore障屏。
 * 读final域的重排序规则要求编译器在读final域的操作前面插入一个LoadLoad屏障。
 * 
 * JSR-133为什么要增强final的语义:
 * 在旧的Java内存模型中 ，最严重的一个缺陷就是线程可能看到final域的值会改变。比如，一个线程当前看到一个整形final域的值为0（还未初始化之前的默认值），
 * 过一段时间之后这个线程再去读这个final域的值时，却发现值变为了1（被某个线程初始化之后的值）。
 * 为了修补这个漏洞，JSR-133增强了final的语义。通过为final域增加写和读重排序规则，可以为java程序员提供初始化安全保证：
 * 只要对象是正确构造的（被构造对象的引用在构造函数中没有“逸出”），那么不需要使用同步（指lock和volatile的使用），
 * 就可以保证任意线程都能看到这个final域在构造函数中被初始化之后的值。
 * 
 * 
 * Java中的volatile关键字：
 *  volatile关键字可以保证直接从主存中读取一个变量，如果这个变量被修改后，总是会被写回到主存中去。Java内存模型是通过在变量修改后将新值同步回主内存，
 *  在变量读取前从主内存刷新变量值这种依赖主内存作为传递媒介的方式来实现可见性的，无论是普通变量还是volatile变量都是如此，
 *  普通变量与volatile变量的区别是：volatile的特殊规则保证了新值能立即同步到主内存，以及每个线程在每次使用volatile变量前都立即从主内存刷新。
 *  因此我们可以说volatile保证了多线程操作时变量的可见性，而普通变量则不能保证这一点。 
 *  
 * Java中的synchronized关键字：
 *  同步快的可见性是由“如果对一个变量执行lock操作，将会清空工作内存中此变量的值，在执行引擎使用这个变量前需要重新执行load或assign操作初始化变量的值”、
 *  “对一个变量执行unlock操作之前，必须先把此变量同步回主内存中（执行store和write操作）”这两条规则获得的。 
 *  
 * Java中的final关键字：
 *  final关键字的可见性是指，被final修饰的字段在构造器中一旦被初始化完成，并且构造器没有把“this”的引用传递出去（this引用逃逸是一件很危险的事情，
 *  其他线程有可能通过这个引用访问到“初始化了一半”的对象），那么在其他线程就能看见final字段的值（无须同步）。
 * 
 * @author       zq
 * @date         2017年9月17日  上午9:35:56
 * Copyright: 	  北京亚信智慧数据科技有限公司
 */
public class JavaMemoryModel {
	 
	int a, b;
	volatile int v, u;

	void readAndWrite() {
		
		int i, j;
		// nomal read
		i = a;	// load a
		j = b;	// load b
		
		// volatile read
		i = v;	// load v
			  	// LoadLoad
		        // 忽略LoadStore，因为后面是个volatile read
		
		j = u;	// load u
		        // 忽略LoadLoad，因为后面指令序列里没有nomal read指令
				// LoadStore
		
		// normal write
		a = i;	// store a
		b = j;	// store b
		
		// volatile write
				// StoreStore
		v = i;	// store v
		        // StoreLoad
		
				// StoreStore
		u = j;	// store u
				// StoreLoad
		        // x86下的汇编代码: 0x00000000029d3044: lock add dword ptr [rsp],0h  ;*putfield u
		        // lock的作用是使得本CPU的Cache写入了内存，该写入动作也会引起别的CPU invalidate其Cache。
		        // 1. 锁住主存 
		        // 2. 任何读必须在写完成之后再执行
		        // 3. 使其它线程这个值的栈缓存失效
	}
}

class X {
	
	int a;
	volatile int v;
	void f() {
		
		int i;
		synchronized (this) { // enter EnterLoad EnterStore
			i = a;// load a
			a = i;// store a
		}// LoadExit StoreExit exit ExitEnter

		synchronized (this) {// enter ExitEnter
			synchronized (this) {// enter
			}// EnterExit exit
		}// ExitExit exit ExitEnter ExitLoad

		i = v;// load v
		synchronized (this) {// LoadEnter enter
		} // exit ExitEnter ExitStore

		v = i; // store v
		synchronized (this) { // StoreEnter enter
		} // EnterExit exit
	}
}

